{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def parse_sql_file(filename):\n",
    "    buffer = []\n",
    "\n",
    "    with open(filename, 'r', encoding=\"latin-1\") as file:\n",
    "        for line in file:\n",
    "            # If we encounter a line with INSERT and the buffer is not empty,\n",
    "            # finalize the previous command\n",
    "    \n",
    "            if \"INSERT\" in line and buffer:\n",
    "                yield \"\".join(buffer).strip()\n",
    "                buffer = []\n",
    "\n",
    "            buffer.append(line)\n",
    "    # If there's any remaining content in the buffer, yield it as a command\n",
    "    if buffer:\n",
    "        yield \"\".join(buffer).strip()\n",
    "    \n",
    "\n",
    "def use_parsed_statement(statement):\n",
    "    print(statement)\n",
    "    exit()\n",
    "# parse the SQL query\n",
    "for statement in parse_sql_file('C:/Workzone/LibgenAssembler/dummy/fiction.sql'):\n",
    "    use_parsed_statement(statement)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy MD5 db generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created at data_md5.db with provided MD5 entries.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Sample data from the dir command output\n",
    "data = [\n",
    "    \"003ae617fbaba357b52f2f0c2b6a21dd\",\n",
    "    \"007c3f53ab5cb74f47626f4eb682347f\",\n",
    "    \"00815a5fbc001507c2ef36dda8e3b0dd\",\n",
    "    \"00acaf02b9dfa6495f4842bd5de2a675\",\n",
    "    \"00c1dbfd61f7a0252de228c415f2ae9c\",\n",
    "    \"00e1cdcd848a1c487bd96ef713024cd0\",\n",
    "    \"013cf8222b4d0961ca03f0a5fcedc488\",\n",
    "    \"015848415929eb612311ee9312d4622a\",\n",
    "    \"017df63a2f23cd875243bb68b64f30f2\",\n",
    "    \"01cea32746e200a8996d1c7aa8456302\",\n",
    "    \"01f73c3b8931af9282a5e69a044f1d58\",\n",
    "    \"0213f27d25e5df58e43b889d94b41be4\",\n",
    "    \"02b4a3b6df4702cf6c58637c0f3568c6\",\n",
    "    \"02b4e17622941606379bf55548420a39\",\n",
    "    \"02c99d4aaf3e25039f5a2191d9f6d2a8\",\n",
    "    \"02e3468a99ed99c9cf491b8ea44dfe27\",\n",
    "    \"03159b30a4d6dd135f2632d673c61377\",\n",
    "    \"03a8df2a8d3e605a1fbb99c5c62ce507\",\n",
    "    \"0406256bc69a1c5bfd476cd51cac58b7\",\n",
    "    \"04146f550256c76760a600db9db090aa\",\n",
    "    \"04158faf7686dd9fe1783bfcf8e93895\",\n",
    "    \"045e1dee8a334e65b4d9d8e8e420d378\",\n",
    "    \"048ea0496db0444f873139cd705a07af\",\n",
    "    \"04b8f4d1ecf885329aa0018758eda72d\",\n",
    "    \"04d9a97c1861cfd4c23b2fbac4efe001\",\n",
    "    \"0506c4375d4add47aa386d2d0f8846d8\",\n",
    "    \"052d4a0a1cf4c5f8723ea7d9cafae8c3\",\n",
    "    \"057e9e95279c84dcb173c7bc5c12671f\",\n",
    "    \"05ffd9ed63bf0521b564119902e7725d\",\n",
    "    \"064da6f14ed4f7f238e0c75e2b36ce64\",\n",
    "    \"06a5e44e285cf4df04ac3c71537c06be\",\n",
    "    \"06ebc131aff35ea06b943967f1bb7f6c\",\n",
    "    \"07bea3da7b8685423b6778e2887ce892\",\n",
    "    \"07da000febe5b13b4ab4993dba9a7884\",\n",
    "    \"07ff165ac7931ad668f73748484a379f\"\n",
    "]\n",
    "\n",
    "\n",
    "def create_and_populate_db(db_path, md5_values):\n",
    "    \"\"\"Create a SQLite database and populate it with provided MD5 values.\"\"\"\n",
    "\n",
    "    # Connect to the database (this will create it if it doesn't exist)\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create the 'compact' table with 'md5' column\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS compact (\n",
    "        md5 TEXT UNIQUE\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "    # Insert provided MD5 values\n",
    "    for md5_value in md5_values:\n",
    "        try:\n",
    "            cursor.execute(\"INSERT INTO compact (md5) VALUES (?)\", (md5_value,))\n",
    "        except sqlite3.IntegrityError:\n",
    "            # In case there are any duplicate MD5 values in the list, this will handle the UNIQUE constraint violation.\n",
    "            pass\n",
    "\n",
    "    # Commit changes and close connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DB_PATH = \"data_md5.db\"\n",
    "    create_and_populate_db(DB_PATH, data)\n",
    "    print(f\"Database created at {DB_PATH} with provided MD5 entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MySQL structure printer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: fiction\n",
      "Columns:\n",
      "\t ID (int unsigned)\n",
      "\t MD5 (char(32))\n",
      "\t Title (varchar(2000))\n",
      "\t Author (varchar(300))\n",
      "\t Series (varchar(300))\n",
      "\t Edition (varchar(50))\n",
      "\t Language (varchar(45))\n",
      "\t Year (varchar(10))\n",
      "\t Publisher (varchar(100))\n",
      "\t Pages (varchar(10))\n",
      "\t Identifier (varchar(400))\n",
      "\t GooglebookID (varchar(45))\n",
      "\t ASIN (varchar(10))\n",
      "\t Coverurl (varchar(200))\n",
      "\t Extension (varchar(10))\n",
      "\t Filesize (int unsigned)\n",
      "\t Library (varchar(50))\n",
      "\t Issue (varchar(100))\n",
      "\t Locator (varchar(512))\n",
      "\t Commentary (varchar(500))\n",
      "\t Generic (char(32))\n",
      "\t Visible (char(3))\n",
      "\t TimeAdded (timestamp)\n",
      "\t TimeLastModified (timestamp)\n",
      "Table: fiction_description\n",
      "Columns:\n",
      "\t MD5 (char(32))\n",
      "\t Descr (mediumtext)\n",
      "\t TimeLastModified (timestamp)\n",
      "Table: fiction_hashes\n",
      "Columns:\n",
      "\t md5 (char(32))\n",
      "\t crc32 (char(8))\n",
      "\t edonkey (char(32))\n",
      "\t aich (char(32))\n",
      "\t sha1 (char(40))\n",
      "\t tth (char(39))\n",
      "\t btih (char(40))\n",
      "\t sha256 (char(64))\n",
      "\t ipfs_cid (char(62))\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "\n",
    "# Connect to the MySQL server\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"Isa66076607.\",\n",
    "    database=\"fiction\"\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Fetch tables\n",
    "cursor.execute(\"SHOW TABLES\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "for table in tables:\n",
    "    table_name = table[0]\n",
    "    print(f\"Table: {table_name}\\nColumns:\")\n",
    "    \n",
    "    cursor.execute(f\"SHOW COLUMNS FROM {table_name}\")\n",
    "    columns = cursor.fetchall()\n",
    "    \n",
    "    for column in columns:\n",
    "        print(f\"\\t {column[0]} ({column[1]})\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sqlite3 structure printer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQLITE_DB_PATH = \"C:\\Workzone\\Datasets\\LibgenDatabases\\merged_fiction_nonfiction_sql3.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: info\n",
      "Columns:\n",
      "\tID (INTEGER UNSIGNED)\n",
      "\tMD5 (CHAR(32))\n",
      "\tTitle (VARCHAR(2000))\n",
      "\tAuthor (VARCHAR(300))\n",
      "\tLanguage (VARCHAR(45))\n",
      "\tType (TEXT)\n",
      "\tYear (VARCHAR(10))\n",
      "\tPages (VARCHAR(10))\n",
      "\tExtension (VARCHAR(10))\n",
      "\tFilesize (INTEGER UNSIGNED)\n",
      "Table: description\n",
      "Columns:\n",
      "\tMD5 (CHAR(32))\n",
      "\tDescription (MEDIUMTEXT)\n",
      "Table: compact\n",
      "Columns:\n",
      "\tMD5 (CHAR(32))\n",
      "\tType (TEXT)\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(SQLITE_DB_PATH)  # Change the path to your SQLite database file if needed\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Fetch tables\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "for table in tables:\n",
    "    table_name = table[0]\n",
    "    print(f\"Table: {table_name}\\nColumns:\")\n",
    "    \n",
    "    cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "    columns = cursor.fetchall()\n",
    "    \n",
    "    for column in columns:\n",
    "        print(f\"\\t{column[1]} ({column[2]})\")  # column[1] is the name, column[2] is the data type\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DB Merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FICTION_DB_PATH = \"C:/Workzone/Datasets/LibgenDatabases/fiction_sql3.db\"\n",
    "NONFICTION_DB_PATH = \"C:/Workzone/Datasets/LibgenDatabases/nonfiction_sql3.db\"\n",
    "MERGED_DB_PATH = \"merged.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Info Table - Fiction: 100%|██████████| 2762624/2762624 [00:17<00:00, 160600.39it/s]\n",
      "Description Table - Fiction:  45%|████▍     | 1241831/2762624 [00:10<00:12, 121808.08it/s]\n",
      "Compact Table - Fiction: 100%|██████████| 2762624/2762624 [00:07<00:00, 393199.49it/s]\n",
      "Info Table - Nonfiction: 100%|██████████| 4106588/4106588 [01:01<00:00, 66795.26it/s]\n",
      "Description Table - Nonfiction:  75%|███████▍  | 3070953/4106588 [00:50<00:16, 61402.48it/s] \n",
      "Compact Table - Nonfiction: 100%|██████████| 4106588/4106588 [00:52<00:00, 78674.82it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_row_count(conn, table_name):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    cursor.close()\n",
    "    return count\n",
    "\n",
    "# Connect to existing databases\n",
    "conn_fiction = sqlite3.connect(FICTION_DB_PATH)\n",
    "conn_nonfiction = sqlite3.connect(NONFICTION_DB_PATH)\n",
    "\n",
    "# Create a new merged database\n",
    "conn_merged = sqlite3.connect(MERGED_DB_PATH)\n",
    "cursor_merged = conn_merged.cursor()\n",
    "\n",
    "# Creating tables in the merged database\n",
    "cursor_merged.execute(\"\"\"\n",
    "CREATE TABLE info (\n",
    "    ID INTEGER UNSIGNED,\n",
    "    MD5 CHAR(32),\n",
    "    Title VARCHAR(2000),\n",
    "    Author VARCHAR(300),\n",
    "    Language VARCHAR(45),\n",
    "    Type TEXT,\n",
    "    Year VARCHAR(10),\n",
    "    Pages VARCHAR(10),\n",
    "    Extension VARCHAR(10),\n",
    "    Filesize INTEGER UNSIGNED\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "cursor_merged.execute(\"\"\"\n",
    "CREATE TABLE description (\n",
    "    MD5 CHAR(32),\n",
    "    Description MEDIUMTEXT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "cursor_merged.execute(\"\"\"\n",
    "CREATE TABLE compact (\n",
    "    MD5 CHAR(32),\n",
    "    Type TEXT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Merge from fiction database\n",
    "fiction_count = get_row_count(conn_fiction, \"fiction\")\n",
    "for row in tqdm(conn_fiction.execute(\"SELECT ID, MD5, Title, Author, Language, Year, Pages, Extension, Filesize FROM fiction\"), total=fiction_count, desc=\"Info Table - Fiction\"):\n",
    "    cursor_merged.execute(\"INSERT INTO info (ID, MD5, Title, Author, Language, Type, Year, Pages, Extension, Filesize) VALUES (?, ?, ?, ?, ?, 'fiction', ?, ?, ?, ?)\", row)\n",
    "\n",
    "for row in tqdm(conn_fiction.execute(\"SELECT MD5, Descr FROM fiction_description\"), total=fiction_count, desc=\"Description Table - Fiction\"):\n",
    "    cursor_merged.execute(\"INSERT INTO description (MD5, Description) VALUES (?, ?)\", row)\n",
    "\n",
    "for row in tqdm(conn_fiction.execute(\"SELECT MD5 FROM fiction\"), total=fiction_count, desc=\"Compact Table - Fiction\"):\n",
    "    cursor_merged.execute(\"INSERT INTO compact (MD5, Type) VALUES (?, 'fiction')\", (row[0],))\n",
    "\n",
    "# Merge from nonfiction database\n",
    "updated_count = get_row_count(conn_nonfiction, \"updated\")\n",
    "for row in tqdm(conn_nonfiction.execute(\"SELECT ID, MD5, Title, Author, Language, Year, Pages, Extension, Filesize FROM updated\"), total=updated_count, desc=\"Info Table - Nonfiction\"):\n",
    "    cursor_merged.execute(\"INSERT INTO info (ID, MD5, Title, Author, Language, Type, Year, Pages, Extension, Filesize) VALUES (?, ?, ?, ?, ?, 'nonfiction', ?, ?, ?, ?)\", row)\n",
    "\n",
    "for row in tqdm(conn_nonfiction.execute(\"SELECT md5, descr FROM description\"), total=updated_count, desc=\"Description Table - Nonfiction\"):\n",
    "    cursor_merged.execute(\"INSERT INTO description (MD5, Description) VALUES (?, ?)\", row)\n",
    "\n",
    "for row in tqdm(conn_nonfiction.execute(\"SELECT MD5 FROM updated\"), total=updated_count, desc=\"Compact Table - Nonfiction\"):\n",
    "    cursor_merged.execute(\"INSERT INTO compact (MD5, Type) VALUES (?, 'nonfiction')\", (row[0],))\n",
    "\n",
    "# Close connections\n",
    "conn_fiction.close()\n",
    "conn_nonfiction.close()\n",
    "conn_merged.commit()\n",
    "conn_merged.close()\n",
    "\n",
    "print(\"Merging completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Verifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiction Info Count: 2762624\n",
      "Fiction Description Count: 1241831\n",
      "Nonfiction Info Count: 4106588\n",
      "Nonfiction Description Count: 3070953\n",
      "Merged Info Count: 6869212\n",
      "Merged Description Count: 4312784\n",
      "Merged Compact Count: 6869212\n",
      "Info table merged successfully!\n",
      "Description table merged successfully!\n",
      "Compact table merged successfully!\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "def get_row_count(conn, table_name):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    cursor.close()\n",
    "    return count\n",
    "\n",
    "# Connect to the databases\n",
    "conn_fiction = sqlite3.connect(FICTION_DB_PATH)\n",
    "conn_nonfiction = sqlite3.connect(NONFICTION_DB_PATH)\n",
    "conn_merged = sqlite3.connect(MERGED_DB_PATH)\n",
    "\n",
    "# Get counts from original databases\n",
    "fiction_info_count = get_row_count(conn_fiction, \"fiction\")\n",
    "fiction_description_count = get_row_count(conn_fiction, \"fiction_description\")\n",
    "\n",
    "nonfiction_info_count = get_row_count(conn_nonfiction, \"updated\")\n",
    "nonfiction_description_count = get_row_count(conn_nonfiction, \"description\")\n",
    "\n",
    "# Get counts from merged database\n",
    "merged_info_count = get_row_count(conn_merged, \"info\")\n",
    "merged_description_count = get_row_count(conn_merged, \"description\")\n",
    "merged_compact_count = merged_info_count # compact table should have one entry per book\n",
    "\n",
    "# Verification\n",
    "print(f\"Fiction Info Count: {fiction_info_count}\")\n",
    "print(f\"Fiction Description Count: {fiction_description_count}\")\n",
    "print(f\"Nonfiction Info Count: {nonfiction_info_count}\")\n",
    "print(f\"Nonfiction Description Count: {nonfiction_description_count}\")\n",
    "print(f\"Merged Info Count: {merged_info_count}\")\n",
    "print(f\"Merged Description Count: {merged_description_count}\")\n",
    "print(f\"Merged Compact Count: {merged_compact_count}\")\n",
    "\n",
    "# Check if counts are consistent\n",
    "if fiction_info_count + nonfiction_info_count == merged_info_count:\n",
    "    print(\"Info table merged successfully!\")\n",
    "else:\n",
    "    print(\"Info table counts mismatch!\")\n",
    "\n",
    "if fiction_description_count + nonfiction_description_count == merged_description_count:\n",
    "    print(\"Description table merged successfully!\")\n",
    "else:\n",
    "    print(\"Description table counts mismatch!\")\n",
    "\n",
    "if merged_info_count == merged_compact_count:\n",
    "    print(\"Compact table merged successfully!\")\n",
    "else:\n",
    "    print(\"Compact table counts mismatch!\")\n",
    "\n",
    "# Close connections\n",
    "conn_fiction.close()\n",
    "conn_nonfiction.close()\n",
    "conn_merged.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Json Element Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 elements in the JSON file.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "FILE_PATH = \"C:/Workzone/LibgenAssembler/outputs/embedder/2023-10-30_23-21-00/batch_5.json\"\n",
    "\n",
    "# Open the JSON file\n",
    "with open(FILE_PATH) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Count the number of elements\n",
    "count = len(data)\n",
    "\n",
    "# Print the count\n",
    "print(f\"There are {count} elements in the JSON file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DUMMY DB CREATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# User-defined variables\n",
    "SOURCE_DB_PATH = \"C:\\Workzone\\Datasets\\LibgenDatabases\\merged_fiction_nonfiction_sql3.db\"\n",
    "DEST_DB_PATH = \"dummy11.db\"\n",
    "\n",
    "# Step 1: Connect to the source SQLite database\n",
    "source_conn = sqlite3.connect(SOURCE_DB_PATH)\n",
    "source_cursor = source_conn.cursor()\n",
    "\n",
    "# Step 2: Connect to the destination SQLite database (or create a new one)\n",
    "dest_conn = sqlite3.connect(DEST_DB_PATH)\n",
    "dest_cursor = dest_conn.cursor()\n",
    "\n",
    "# Step 3: Copy the table structure of 'info' and 'description'\n",
    "for table in ['info', 'description']:\n",
    "    create_stmt = source_cursor.execute(f'''SELECT sql FROM sqlite_master WHERE type='table' AND name='{table}';''').fetchone()[0]\n",
    "    dest_cursor.execute(create_stmt)\n",
    "\n",
    "# Step 4: Copy the first 20,000 rows from 'info'\n",
    "rows_info = source_cursor.execute('SELECT * FROM info LIMIT 20000;').fetchall()\n",
    "\n",
    "# Create the appropriate number of placeholders based on the number of columns\n",
    "placeholders_info = ', '.join(['?'] * len(rows_info[0]))\n",
    "dest_cursor.executemany(f'INSERT INTO info VALUES ({placeholders_info});', rows_info)\n",
    "\n",
    "# Step 5: Extract MD5 values from the copied rows in 'info' of destination database\n",
    "md5_values = dest_cursor.execute('SELECT MD5 FROM info;').fetchall()\n",
    "md5_values = tuple([item[0] for item in md5_values])\n",
    "\n",
    "# Step 6: Fetch matching rows from 'description' in source database using the MD5 values\n",
    "placeholders = ', '.join(['?'] * len(md5_values))\n",
    "rows_description = source_cursor.execute(f'SELECT * FROM description WHERE MD5 IN ({placeholders});', md5_values).fetchall()\n",
    "\n",
    "# Create the appropriate number of placeholders based on the number of columns\n",
    "placeholders_description = ', '.join(['?'] * len(rows_description[0]))\n",
    "dest_cursor.executemany(f'INSERT INTO description VALUES ({placeholders_description});', rows_description)\n",
    "\n",
    "# Step 7: Commit and close the connections\n",
    "dest_conn.commit()\n",
    "source_conn.close()\n",
    "dest_conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL lite lenght extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(\"C:\\\\Workzone\\\\Datasets\\\\LibgenDatabases\\\\merged_fiction_nonfiction_sql3.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Query to get all descriptions and their MD5 values\n",
    "cursor.execute(\"SELECT MD5, Description FROM description\")\n",
    "lenghts= []\n",
    "\n",
    "for row in cursor.fetchall():\n",
    "    md5, description = row\n",
    "    if description:  # check if description is not None or empty\n",
    "        tokenized_description = sent_tokenize(description)\n",
    "\n",
    "        for token in tokenized_description:\n",
    "            \n",
    "            lenghts.append(len(token))\n",
    "\n",
    "            \n",
    "\n",
    "        #   md5_length_dict[md5] = len(description)\n",
    "\n",
    "# Save the dictionary to a .npy file\n",
    "np.save(\".description_lengths.npy\", lenghts)\n",
    "\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliter Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "class TextSplitter:\n",
    "    def __init__(self, batch_sentence_count, max_sentence_character_count):\n",
    "        self.batch_sentence_count = batch_sentence_count\n",
    "        self.max_sentence_character_count = max_sentence_character_count\n",
    "\n",
    "    def split_to_batches(self, text):\n",
    "        tokenized_description = sent_tokenize(text)\n",
    "        \n",
    "        # Check for max_sentence_character_count and split if necessary\n",
    "        processed_sentences = []\n",
    "        for sentence in tokenized_description:\n",
    "            while len(sentence) > self.max_sentence_character_count:\n",
    "                # Split the sentence\n",
    "                processed_sentences.append(sentence[:self.max_sentence_character_count])\n",
    "                sentence = sentence[self.max_sentence_character_count:]\n",
    "            processed_sentences.append(sentence)\n",
    "        \n",
    "        # Group sentences to batches\n",
    "        batches = []\n",
    "        batch = []\n",
    "        for sentence in processed_sentences:\n",
    "            if len(batch) < self.batch_sentence_count:\n",
    "                batch.append(sentence)\n",
    "            else:\n",
    "                batches.append(' '.join(batch))\n",
    "                batch = [sentence]\n",
    "        if batch:  # Append the last batch if it's not empty\n",
    "            batches.append(' '.join(batch))\n",
    "        \n",
    "        return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1\n",
      "Expected:\n",
      " This is a simple sentence. Here's another one. This one is a bit longer but should still fit.\n",
      "However, this particular sentence is intentionally made very long so that it exceeds the max_sentence_character_count and\n",
      "needs to be split. Another simple sentence. Yet another one.\n",
      "Got:\n",
      " This is a simple sentence. Here's another one. This one is a bit longer but should still fit.\n",
      "However, this particular sentence is intentionally  made very long so that it exceeds the max_sentenc e_character_count and needs to be split.\n",
      "Another simple sentence. Yet another one.\n",
      "--------------------------------------------------\n",
      "Test 2\n",
      "Expected:\n",
      " Short sentence. Another one.\n",
      "Yet another short one. And one more.\n",
      "The last one.\n",
      "Got:\n",
      " Short sentence. Another one.\n",
      "Yet another short one. And one more.\n",
      "The last one.\n",
      "--------------------------------------------------\n",
      "Test 3\n",
      "Expected:\n",
      " This is a \n",
      "test. A small\n",
      " test. Another\n",
      " tiny test. Yet\n",
      " one more. Last\n",
      " test.\n",
      "Got:\n",
      " This is a  test. A small te st. Another ti\n",
      "ny test. Yet one mo re. Last test.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def test_text_splitter():\n",
    "    # Test 1\n",
    "    text_splitter1 = TextSplitter(3, 50)\n",
    "    test_text1 = \"This is a simple sentence. Here's another one. This one is a bit longer but should still fit. However, this particular sentence is intentionally made very long so that it exceeds the max_sentence_character_count and needs to be split. Another simple sentence. Yet another one.\"\n",
    "    result1 = text_splitter1.split_to_batches(test_text1)\n",
    "    expected1 = [\n",
    "        \"This is a simple sentence. Here's another one. This one is a bit longer but should still fit.\",\n",
    "        \"However, this particular sentence is intentionally made very long so that it exceeds the max_sentence_character_count and\",\n",
    "        \"needs to be split. Another simple sentence. Yet another one.\"\n",
    "    ]\n",
    "    print(\"Test 1\")\n",
    "    print(\"Expected:\\n\", \"\\n\".join(expected1))\n",
    "    print(\"Got:\\n\", \"\\n\".join(result1))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Test 2\n",
    "    text_splitter2 = TextSplitter(2, 30)\n",
    "    test_text2 = \"Short sentence. Another one. Yet another short one. And one more. The last one.\"\n",
    "    result2 = text_splitter2.split_to_batches(test_text2)\n",
    "    expected2 = [\n",
    "        \"Short sentence. Another one.\",\n",
    "        \"Yet another short one. And one more.\",\n",
    "        \"The last one.\"\n",
    "    ]\n",
    "    print(\"Test 2\")\n",
    "    print(\"Expected:\\n\", \"\\n\".join(expected2))\n",
    "    print(\"Got:\\n\", \"\\n\".join(result2))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Test 3\n",
    "    text_splitter3 = TextSplitter(5, 10)\n",
    "    test_text3 = \"This is a test. A small test. Another tiny test. Yet one more. Last test.\"\n",
    "    result3 = text_splitter3.split_to_batches(test_text3)\n",
    "    expected3 = [\n",
    "        \"This is a \",\n",
    "        \"test. A small\",\n",
    "        \" test. Another\",\n",
    "        \" tiny test. Yet\",\n",
    "        \" one more. Last\",\n",
    "        \" test.\"\n",
    "    ]\n",
    "    print(\"Test 3\")\n",
    "    print(\"Expected:\\n\", \"\\n\".join(expected3))\n",
    "    print(\"Got:\\n\", \"\\n\".join(result3))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "test_text_splitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jina GPU Memory tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
